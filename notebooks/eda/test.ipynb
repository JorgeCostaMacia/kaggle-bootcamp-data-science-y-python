{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Carga librerías <====================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Transformación de datos\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "import category_encoders as ce\n",
    "\n",
    "# Modelos\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Seleccion de variables y tuning de hiperparámetros\n",
    "from sklearn.feature_selection import RFECV, RFE\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "\n",
    "# Métricas para evaluar un modelo de clasificación\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_curve, roc_auc_score, average_precision_score, confusion_matrix\n",
    "\n",
    "# Librerías para visualización de resultados\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Tratamiento de datos\n",
    "# ------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Gráficos\n",
    "# ------------------------------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocesado y modelado\n",
    "# ------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_graphviz, export_text\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.metrics import accuracy_score, confusion_matrix, auc, plot_roc_curve, roc_curve, classification_report\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, auc, RocCurveDisplay , roc_curve, classification_report\n",
    "\n",
    "# Para que no se corten el listado de filas y columnas al ejecutar instrucciones\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Carga Datos <========================================================\n",
    "\n",
    "datos = pd.read_csv('../../data/raw/test.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Carga Procesados Train <=============================================\n",
    "\n",
    "cat_cols = pickle.load(open(\"../../columns/cat.pkl\", 'rb'))\n",
    "cat_cols_mask_3 = pickle.load(open(\"../../columns/cat_mask_3.pkl\", 'rb'))\n",
    "cat_cols_mask_4 = pickle.load(open(\"../../columns/cat_mask_4.pkl\", 'rb'))\n",
    "cat_cols_mask_5 = pickle.load(open(\"../../columns/cat_mask_5.pkl\", 'rb'))\n",
    "cat_cols_mask_6 = pickle.load(open(\"../../columns/cat_mask_6.pkl\", 'rb'))\n",
    "cat_cols_target_encoder = pickle.load(open(\"../../columns/cat_target_encoder.pkl\", 'rb'))\n",
    "cat_cols_onehot_encoder = pickle.load(open(\"../../columns/cat_onehot_encoder.pkl\", 'rb'))\n",
    "cat_cols_pro = pickle.load(open(\"../../columns/cat_pro.pkl\", 'rb'))\n",
    "\n",
    "num_cols = pickle.load(open(\"../../columns/num.pkl\", 'rb'))\n",
    "num_cols_onehot_encoder = pickle.load(open(\"../../columns/num_onehot_encoder.pkl\", 'rb'))\n",
    "num_cols_correlacion = pickle.load(open(\"../../columns/num_correlacion.pkl\", 'rb'))\n",
    "num_cols_pro = pickle.load(open(\"../../columns/num_pro.pkl\", 'rb'))\n",
    "\n",
    "target_encoder = pickle.load(open(\"../../encoders/target.pkl\", 'rb'))\n",
    "onehot_encoder = pickle.load(open(\"../../encoders/onehot.pkl\", 'rb'))\n",
    "\n",
    "intercuartilico = pickle.load(open(\"../../interquartiles/intercuartilico.pkl\", 'rb'))\n",
    "\n",
    "cat_simple_imputer = pickle.load(open(\"../../imputers/cat_simple.pkl\", 'rb'))\n",
    "num_simple_imputer = pickle.load(open(\"../../imputers/num_simple.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Definir variables <==================================================\n",
    "\n",
    "IDENTIFIER = \"MachineIdentifier\"\n",
    "LABEL = \"HasDetections\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Eliminar columnas nuevas <===========================================\n",
    "\n",
    "datos = datos[[IDENTIFIER]+cat_cols+num_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Separar datos por tipos categoricas <================================\n",
    "\n",
    "datos[cat_cols] = datos[cat_cols].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Separar datos por tipos numericas <==================================\n",
    "\n",
    "datos[num_cols] = datos[num_cols].astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Imputar nulos para tipos categoricas <===============================\n",
    "\n",
    "datos[cat_cols] = cat_simple_imputer.transform(datos[cat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Procesar mascaras y versiones 3 partes <=============================\n",
    "\n",
    "for col in cat_cols_mask_3:\n",
    "    datos[[col + \"_1\", col + \"_2\", col + \"_3\"]] = datos[col].str.split(\".\", expand=True)\n",
    "\n",
    "for col in cat_cols_mask_3:\n",
    "    cat_cols.remove(col)\n",
    "    cat_cols.append(col + \"_1\")\n",
    "    cat_cols.append(col + \"_2\")\n",
    "    cat_cols.append(col + \"_3\")\n",
    "\n",
    "datos.drop(columns=cat_cols_mask_3, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Procesar mascaras y versiones 4 partes <=============================\n",
    "\n",
    "for col in cat_cols_mask_4:\n",
    "    datos[[col + \"_1\", col + \"_2\", col + \"_3\", col + \"_4\"]] = datos[col].str.split(\".\", expand=True)\n",
    "\n",
    "for col in cat_cols_mask_4:\n",
    "    cat_cols.remove(col)\n",
    "    cat_cols.append(col + \"_1\")\n",
    "    cat_cols.append(col + \"_2\")\n",
    "    cat_cols.append(col + \"_3\")\n",
    "    cat_cols.append(col + \"_4\")    \n",
    "\n",
    "datos.drop(columns=cat_cols_mask_4, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Procesar mascaras y versiones 5 partes <=============================\n",
    "\n",
    "for col in cat_cols_mask_5:\n",
    "    datos[[col + \"_1\", col + \"_2\", col + \"_3\", col + \"_4\", col + \"_5\"]] = datos[col].str.split(\".\", expand=True)\n",
    "\n",
    "for col in cat_cols_mask_5:\n",
    "    cat_cols.remove(col)\n",
    "    cat_cols.append(col + \"_1\")\n",
    "    cat_cols.append(col + \"_2\")\n",
    "    cat_cols.append(col + \"_3\")\n",
    "    cat_cols.append(col + \"_4\")\n",
    "    cat_cols.append(col + \"_5\")\n",
    "\n",
    "datos.drop(columns=cat_cols_mask_5, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Procesar mascaras y versiones 6 partes <=============================\n",
    "\n",
    "for col in cat_cols_mask_6:\n",
    "    datos[[col + \"_1\", col + \"_2\", col + \"_3\", col + \"_4\", col + \"_5\", col + \"_6\"]] = datos[col].str.split(\".\", expand=True)\n",
    "\n",
    "for col in cat_cols_mask_6:\n",
    "    cat_cols.remove(col)\n",
    "    cat_cols.append(col + \"_1\")\n",
    "    cat_cols.append(col + \"_2\")\n",
    "    cat_cols.append(col + \"_3\")\n",
    "    cat_cols.append(col + \"_4\")\n",
    "    cat_cols.append(col + \"_5\")\n",
    "    cat_cols.append(col + \"_6\")\n",
    "\n",
    "datos.drop(columns=cat_cols_mask_6, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Procesar target encoder <============================================\n",
    "\n",
    "datos[cat_cols_target_encoder] = target_encoder.transform(datos[cat_cols_target_encoder])\n",
    "\n",
    "for col in cat_cols_target_encoder:\n",
    "    cat_cols.remove(col)\n",
    "    num_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcosta\\AppData\\Local\\Temp\\ipykernel_30924\\4241278943.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datos[num_cols_onehot_encoder_test] = pd.DataFrame(onehot_datos, columns=num_cols_onehot_encoder_test)[num_cols_onehot_encoder_test]\n",
      "C:\\Users\\jcosta\\AppData\\Local\\Temp\\ipykernel_30924\\4241278943.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datos[num_cols_onehot_encoder_test] = pd.DataFrame(onehot_datos, columns=num_cols_onehot_encoder_test)[num_cols_onehot_encoder_test]\n",
      "C:\\Users\\jcosta\\AppData\\Local\\Temp\\ipykernel_30924\\4241278943.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datos[num_cols_onehot_encoder_test] = pd.DataFrame(onehot_datos, columns=num_cols_onehot_encoder_test)[num_cols_onehot_encoder_test]\n",
      "C:\\Users\\jcosta\\AppData\\Local\\Temp\\ipykernel_30924\\4241278943.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datos[num_cols_onehot_encoder_test] = pd.DataFrame(onehot_datos, columns=num_cols_onehot_encoder_test)[num_cols_onehot_encoder_test]\n",
      "C:\\Users\\jcosta\\AppData\\Local\\Temp\\ipykernel_30924\\4241278943.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datos[num_cols_onehot_encoder_test] = pd.DataFrame(onehot_datos, columns=num_cols_onehot_encoder_test)[num_cols_onehot_encoder_test]\n",
      "C:\\Users\\jcosta\\AppData\\Local\\Temp\\ipykernel_30924\\4241278943.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datos[num_cols_onehot_encoder_test] = pd.DataFrame(onehot_datos, columns=num_cols_onehot_encoder_test)[num_cols_onehot_encoder_test]\n",
      "C:\\Users\\jcosta\\AppData\\Local\\Temp\\ipykernel_30924\\4241278943.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datos[num_cols_onehot_encoder_test] = pd.DataFrame(onehot_datos, columns=num_cols_onehot_encoder_test)[num_cols_onehot_encoder_test]\n",
      "C:\\Users\\jcosta\\AppData\\Local\\Temp\\ipykernel_30924\\4241278943.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datos[num_cols_onehot_encoder_test] = pd.DataFrame(onehot_datos, columns=num_cols_onehot_encoder_test)[num_cols_onehot_encoder_test]\n",
      "C:\\Users\\jcosta\\AppData\\Local\\Temp\\ipykernel_30924\\4241278943.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datos[num_cols_onehot_encoder_test] = pd.DataFrame(onehot_datos, columns=num_cols_onehot_encoder_test)[num_cols_onehot_encoder_test]\n",
      "C:\\Users\\jcosta\\AppData\\Local\\Temp\\ipykernel_30924\\4241278943.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datos[num_cols_onehot_encoder_test] = pd.DataFrame(onehot_datos, columns=num_cols_onehot_encoder_test)[num_cols_onehot_encoder_test]\n",
      "C:\\Users\\jcosta\\AppData\\Local\\Temp\\ipykernel_30924\\4241278943.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datos[num_cols_onehot_encoder_test] = pd.DataFrame(onehot_datos, columns=num_cols_onehot_encoder_test)[num_cols_onehot_encoder_test]\n",
      "C:\\Users\\jcosta\\AppData\\Local\\Temp\\ipykernel_30924\\4241278943.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datos[num_cols_onehot_encoder_test] = pd.DataFrame(onehot_datos, columns=num_cols_onehot_encoder_test)[num_cols_onehot_encoder_test]\n",
      "C:\\Users\\jcosta\\AppData\\Local\\Temp\\ipykernel_30924\\4241278943.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datos[num_cols_onehot_encoder_test] = pd.DataFrame(onehot_datos, columns=num_cols_onehot_encoder_test)[num_cols_onehot_encoder_test]\n",
      "C:\\Users\\jcosta\\AppData\\Local\\Temp\\ipykernel_30924\\4241278943.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datos[num_cols_onehot_encoder_test] = pd.DataFrame(onehot_datos, columns=num_cols_onehot_encoder_test)[num_cols_onehot_encoder_test]\n",
      "C:\\Users\\jcosta\\AppData\\Local\\Temp\\ipykernel_30924\\4241278943.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datos[num_cols_onehot_encoder_test] = pd.DataFrame(onehot_datos, columns=num_cols_onehot_encoder_test)[num_cols_onehot_encoder_test]\n",
      "C:\\Users\\jcosta\\AppData\\Local\\Temp\\ipykernel_30924\\4241278943.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datos[num_cols_onehot_encoder_test] = pd.DataFrame(onehot_datos, columns=num_cols_onehot_encoder_test)[num_cols_onehot_encoder_test]\n",
      "C:\\Users\\jcosta\\AppData\\Local\\Temp\\ipykernel_30924\\4241278943.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datos[num_cols_onehot_encoder_test] = pd.DataFrame(onehot_datos, columns=num_cols_onehot_encoder_test)[num_cols_onehot_encoder_test]\n",
      "C:\\Users\\jcosta\\AppData\\Local\\Temp\\ipykernel_30924\\4241278943.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datos[num_cols_onehot_encoder_test] = pd.DataFrame(onehot_datos, columns=num_cols_onehot_encoder_test)[num_cols_onehot_encoder_test]\n",
      "C:\\Users\\jcosta\\AppData\\Local\\Temp\\ipykernel_30924\\4241278943.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datos[num_cols_onehot_encoder_test] = pd.DataFrame(onehot_datos, columns=num_cols_onehot_encoder_test)[num_cols_onehot_encoder_test]\n",
      "C:\\Users\\jcosta\\AppData\\Local\\Temp\\ipykernel_30924\\4241278943.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  datos[num_cols_onehot_encoder_test] = pd.DataFrame(onehot_datos, columns=num_cols_onehot_encoder_test)[num_cols_onehot_encoder_test]\n"
     ]
    }
   ],
   "source": [
    "# ===========================================> Procesar onehot encoder <============================================\n",
    "\n",
    "onehot_datos = onehot_encoder.transform(datos[cat_cols_onehot_encoder]).toarray()\n",
    "num_cols_onehot_encoder_test = onehot_encoder.get_feature_names_out(cat_cols_onehot_encoder)\n",
    "\n",
    "datos[num_cols_onehot_encoder_test] = pd.DataFrame(onehot_datos, columns=num_cols_onehot_encoder_test)[num_cols_onehot_encoder_test]\n",
    "\n",
    "datos.drop(columns=cat_cols_onehot_encoder, inplace=True)\n",
    "\n",
    "for col in cat_cols_onehot_encoder:\n",
    "    cat_cols.remove(col)\n",
    "\n",
    "for col in num_cols_onehot_encoder_test:\n",
    "    num_cols.append(col)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Imputar nulos para tipos numericas <=================================\n",
    "\n",
    "datos[num_cols] = num_simple_imputer.transform(datos[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Tratamiento de valores atípicos <====================================\n",
    "\n",
    "#for col in num_cols:\n",
    "#    datos[col] = datos[col].apply(lambda x: x if intercuartilico[col][\"lower\"] <= x <= intercuartilico[col][\"upper\"] else intercuartilico[col][\"mean\"])\n",
    "\n",
    "#for col in num_cols:\n",
    "#    datos[col] = datos[col].apply(lambda x: None if x < intercuartilico[col][\"lower\"] or x > intercuartilico[col][\"upper\"] else x)\n",
    "\n",
    "#datos[num_cols] = datos[num_cols][~((datos[num_cols] < (intercuartilico[\"Q1\"] - 1.5 * intercuartilico[\"IQR\"])) |(datos[num_cols] > (intercuartilico[\"Q3\"] + 1.5 * intercuartilico[\"IQR\"]))).any(axis=1)]\n",
    "#datos[num_cols] = datos[num_cols][~((datos[num_cols] < (intercuartilico[\"Q1\"] - 1.5 * intercuartilico[\"IQR\"])) |(datos[num_cols] > (intercuartilico[\"Q3\"] + 1.5 * intercuartilico[\"IQR\"])))].any(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Imputar nulos para tipos numericas <=================================\n",
    "\n",
    "#num_simple_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "#num_simple_imputer = num_simple_imputer.fit(datos[num_cols])\n",
    "#datos[num_cols] = num_simple_imputer.transform(datos[num_cols])\n",
    "\n",
    "#datos[num_cols].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Eliminar por Matriz de correlación <=================================\n",
    "\n",
    "datos.drop(columns=num_cols_correlacion, inplace=True)\n",
    "\n",
    "for col in num_cols_correlacion:\n",
    "    num_cols.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Reiniciar Separar datos por tipos categoricas <======================\n",
    "\n",
    "cat_cols = cat_cols_pro\n",
    "\n",
    "datos[cat_cols] = datos[cat_cols].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Reiniciar Separar datos por tipos numericas <========================\n",
    "\n",
    "num_cols = num_cols_pro\n",
    "\n",
    "datos[num_cols] = datos[num_cols].astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Ordenar variables <==================================================\n",
    "\n",
    "datos = datos[[IDENTIFIER]+cat_cols+num_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Guardamos los datos preprocesados <==================================\n",
    "\n",
    "datos.to_csv('../../data/processed/test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
