{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Carga librerías <====================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Transformación de datos\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "import category_encoders as ce\n",
    "\n",
    "# Modelos\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Seleccion de variables y tuning de hiperparámetros\n",
    "from sklearn.feature_selection import RFECV, RFE\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "\n",
    "# Métricas para evaluar un modelo de clasificación\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_curve, roc_auc_score, average_precision_score, confusion_matrix\n",
    "\n",
    "# Librerías para visualización de resultados\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Tratamiento de datos\n",
    "# ------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Gráficos\n",
    "# ------------------------------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocesado y modelado\n",
    "# ------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_graphviz, export_text\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.metrics import accuracy_score, confusion_matrix, auc, plot_roc_curve, roc_curve, classification_report\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, auc, RocCurveDisplay , roc_curve, classification_report\n",
    "\n",
    "# Para que no se corten el listado de filas y columnas al ejecutar instrucciones\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Carga Datos <========================================================\n",
    "\n",
    "datos = pd.read_csv('../data/test.csv', low_memory=False)\n",
    "\n",
    "cat_cols = pickle.load(open(\"../eda/cat_cols_raw.pkl\", 'rb'))\n",
    "numeric_cols = pickle.load(open(\"../eda/numeric_cols_raw.pkl\", 'rb'))\n",
    "\n",
    "target_encoder_cat = pickle.load(open(\"../eda/target_encoder_cat.pkl\", 'rb'))\n",
    "\n",
    "intercuartilico = pickle.load(open(\"../eda/intercuartilico.pkl\", 'rb'))\n",
    "imp_cat = pickle.load(open(\"../eda/symple_imputer_cat.pkl\", 'rb'))\n",
    "imp_num = pickle.load(open(\"../eda/symple_imputer_num.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos[\"HasDetections\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Eliminar valores nulos <=============================================\n",
    "\n",
    "drop_cols_min_nulls = 0.7\n",
    "drop_cols_nulls = []\n",
    "\n",
    "for col in datos.columns:\n",
    "    if datos[col].isna().sum() / len(datos) >= drop_cols_min_nulls:\n",
    "        drop_cols_nulls.append(col)\n",
    "\n",
    "datos.drop(columns=drop_cols_nulls, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Separar datos por tipos categoricas <================================\n",
    "\n",
    "datos[cat_cols] = datos[cat_cols].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Separar datos por tipos numericas <==================================\n",
    "\n",
    "datos[numeric_cols] = datos[numeric_cols].astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Imputar nulos para tipos categoricas <===============================\n",
    "\n",
    "datos[cat_cols] = imp_cat.transform(datos[cat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Procesar mascaras y versiones 3 partes <=============================\n",
    "\n",
    "mask_cols_3 = []\n",
    "\n",
    "for col in cat_cols:\n",
    "    if datos[col].notnull().all() and datos[col].astype(str).apply(lambda x: x.count('.') == 2).all():\n",
    "        mask_cols_3.append(col)\n",
    "\n",
    "for col in mask_cols_3:\n",
    "    datos[[col + \"_1\", col + \"_2\", col + \"_3\"]] = datos[col].str.split(\".\", expand=True)\n",
    "\n",
    "for col in mask_cols_3:\n",
    "    cat_cols.remove(col)\n",
    "    cat_cols.append(col + \"_1\")\n",
    "    cat_cols.append(col + \"_2\")\n",
    "    cat_cols.append(col + \"_3\")\n",
    "\n",
    "datos.drop(columns=mask_cols_3, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EngineVersion', 'AppVersion', 'AvSigVersion', 'OsVer', 'Census_OSVersion']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===========================================> Procesar mascaras y versiones 4 partes <=============================\n",
    "\n",
    "mask_cols_4 = []\n",
    "\n",
    "for col in cat_cols:\n",
    "    if datos[col].notnull().all() and datos[col].astype(str).apply(lambda x: x.count('.') == 3).all():\n",
    "        mask_cols_4.append(col)\n",
    "\n",
    "for col in mask_cols_4:\n",
    "    datos[[col + \"_1\", col + \"_2\", col + \"_3\", col + \"_4\"]] = datos[col].str.split(\".\", expand=True)\n",
    "\n",
    "for col in mask_cols_4:\n",
    "    cat_cols.remove(col)\n",
    "    cat_cols.append(col + \"_1\")\n",
    "    cat_cols.append(col + \"_2\")\n",
    "    cat_cols.append(col + \"_3\")\n",
    "    cat_cols.append(col + \"_4\")\n",
    "\n",
    "datos.drop(columns=mask_cols_4, inplace=True)\n",
    "\n",
    "mask_cols_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Procesar mascaras y versiones 5 partes <=============================\n",
    "\n",
    "mask_cols_5 = []\n",
    "\n",
    "for col in cat_cols:\n",
    "    if datos[col].notnull().all() and datos[col].astype(str).apply(lambda x: x.count('.') == 4).all():\n",
    "        mask_cols_5.append(col)\n",
    "\n",
    "for col in mask_cols_5:\n",
    "    datos[[col + \"_1\", col + \"_2\", col + \"_3\", col + \"_4\", col + \"_5\"]] = datos[col].str.split(\".\", expand=True)\n",
    "\n",
    "for col in mask_cols_5:\n",
    "    cat_cols.remove(col)\n",
    "    cat_cols.append(col + \"_1\")\n",
    "    cat_cols.append(col + \"_2\")\n",
    "    cat_cols.append(col + \"_3\")\n",
    "    cat_cols.append(col + \"_4\")\n",
    "    cat_cols.append(col + \"_5\")\n",
    "\n",
    "datos.drop(columns=mask_cols_5, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Procesar mascaras y versiones 6 partes <=============================\n",
    "\n",
    "mask_cols_6 = []\n",
    "\n",
    "for col in cat_cols:\n",
    "    if datos[col].notnull().all() and datos[col].astype(str).apply(lambda x: x.count('.') == 5).all():\n",
    "        mask_cols_6.append(col)\n",
    "\n",
    "for c in mask_cols_6:\n",
    "    datos[[col + \"_1\", col + \"_2\", col + \"_3\", col + \"_4\", col + \"_5\", col + \"_6\"]] = datos[col].str.split(\".\", expand=True)\n",
    "\n",
    "for col in mask_cols_6:\n",
    "    cat_cols.remove(col)\n",
    "    cat_cols.append(col + \"_1\")\n",
    "    cat_cols.append(col + \"_2\")\n",
    "    cat_cols.append(col + \"_3\")\n",
    "    cat_cols.append(col + \"_4\")\n",
    "    cat_cols.append(col + \"_5\")\n",
    "    cat_cols.append(col + \"_6\")\n",
    "\n",
    "datos.drop(columns=mask_cols_6, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- AppVersion_2\n- AppVersion_3\n- AppVersion_4\n- AvSigVersion_2\n- AvSigVersion_3\n- ...\nFeature names seen at fit time, yet now missing:\n- AppVersion\n- AutoSampleOptIn\n- AvSigVersion\n- Census_DeviceFamily\n- Census_GenuineStateName\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[151], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m cat_cols_target_encoder:\n\u001b[0;32m     11\u001b[0m     cat_cols\u001b[39m.\u001b[39mremove(col)\n\u001b[1;32m---> 13\u001b[0m encoded_data_cat_target_encoder \u001b[39m=\u001b[39m target_encoder_cat\u001b[39m.\u001b[39;49mtransform(datos[cat_cols_target_encoder])\n\u001b[0;32m     14\u001b[0m encoded_data_cat_target_encoder_test \u001b[39m=\u001b[39m target_encoder_cat\u001b[39m.\u001b[39mtransform(X_test[cat_cols_target_encoder])\n\u001b[0;32m     16\u001b[0m datos \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([datos\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39mcat_cols_target_encoder), encoded_data_cat_target_encoder], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\impute\\_base.py:551\u001b[0m, in \u001b[0;36mSimpleImputer.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Impute all missing values in `X`.\u001b[39;00m\n\u001b[0;32m    537\u001b[0m \n\u001b[0;32m    538\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[39m    `X` with imputed values.\u001b[39;00m\n\u001b[0;32m    548\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    549\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 551\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_input(X, in_fit\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    552\u001b[0m statistics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatistics_\n\u001b[0;32m    554\u001b[0m \u001b[39mif\u001b[39;00m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m statistics\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\impute\\_base.py:344\u001b[0m, in \u001b[0;36mSimpleImputer._validate_input\u001b[1;34m(self, X, in_fit)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[39mraise\u001b[39;00m new_ve \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    343\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 344\u001b[0m         \u001b[39mraise\u001b[39;00m ve\n\u001b[0;32m    346\u001b[0m \u001b[39mif\u001b[39;00m in_fit:\n\u001b[0;32m    347\u001b[0m     \u001b[39m# Use the dtype seen in `fit` for non-`fit` conversion\u001b[39;00m\n\u001b[0;32m    348\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_dtype \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mdtype\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\impute\\_base.py:327\u001b[0m, in \u001b[0;36mSimpleImputer._validate_input\u001b[1;34m(self, X, in_fit)\u001b[0m\n\u001b[0;32m    324\u001b[0m     force_all_finite \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 327\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    328\u001b[0m         X,\n\u001b[0;32m    329\u001b[0m         reset\u001b[39m=\u001b[39;49min_fit,\n\u001b[0;32m    330\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    331\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    332\u001b[0m         force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m    333\u001b[0m         copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy,\n\u001b[0;32m    334\u001b[0m     )\n\u001b[0;32m    335\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m ve:\n\u001b[0;32m    336\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcould not convert\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(ve):\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\base.py:548\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_data\u001b[39m(\n\u001b[0;32m    484\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    485\u001b[0m     X\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mno_validation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params,\n\u001b[0;32m    490\u001b[0m ):\n\u001b[0;32m    491\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    492\u001b[0m \n\u001b[0;32m    493\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[39m        validated.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 548\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_feature_names(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[0;32m    550\u001b[0m     \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_tags()[\u001b[39m\"\u001b[39m\u001b[39mrequires_y\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    551\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    552\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m estimator \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    553\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrequires y to be passed, but the target y is None.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    554\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\base.py:481\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m missing_names \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    477\u001b[0m     message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    478\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    479\u001b[0m     )\n\u001b[1;32m--> 481\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- AppVersion_2\n- AppVersion_3\n- AppVersion_4\n- AvSigVersion_2\n- AvSigVersion_3\n- ...\nFeature names seen at fit time, yet now missing:\n- AppVersion\n- AutoSampleOptIn\n- AvSigVersion\n- Census_DeviceFamily\n- Census_GenuineStateName\n- ...\n"
     ]
    }
   ],
   "source": [
    "# ===========================================> Procesar target encoder <============================================\n",
    "\n",
    "cat_cols_target_encoder_min = 5\n",
    "cat_cols_target_encoder = []\n",
    "\n",
    "for col in cat_cols:\n",
    "    if datos[col].nunique() > cat_cols_target_encoder_min:\n",
    "        cat_cols_target_encoder.append(col)\n",
    "\n",
    "for col in cat_cols_target_encoder:\n",
    "    cat_cols.remove(col)\n",
    "\n",
    "encoded_data_cat_target_encoder = target_encoder_cat.transform(datos[cat_cols_target_encoder])\n",
    "encoded_data_cat_target_encoder_test = target_encoder_cat.transform(X_test[cat_cols_target_encoder])\n",
    "\n",
    "datos = pd.concat([datos.drop(columns=cat_cols_target_encoder), encoded_data_cat_target_encoder], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Procesar onehot encoder <============================================\n",
    "\n",
    "cat_cols_onehot_encoder_max = 5\n",
    "cat_cols_onehot_encoder = []\n",
    "\n",
    "onehot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "for col in cat_cols:\n",
    "    if datos[col].nunique() <= cat_cols_onehot_encoder_max:\n",
    "        cat_cols_onehot_encoder.append(col)\n",
    "\n",
    "for col in cat_cols_onehot_encoder:\n",
    "    cat_cols.remove(col)\n",
    "\n",
    "encoded_cols_onehot_encoder = onehot_encoder.fit_transform(datos[cat_cols_onehot_encoder]).toarray()\n",
    "encoded_data_cat_cols_onehot_encoder = pd.DataFrame(encoded_cols_onehot_encoder, columns=onehot_encoder.get_feature_names_out(cat_cols_onehot_encoder))\n",
    "\n",
    "datos = pd.concat([datos.drop(columns=cat_cols_onehot_encoder), encoded_data_cat_cols_onehot_encoder], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Reiniciar Separar datos por tipos categoricas <======================\n",
    "\n",
    "cat_cols = datos.select_dtypes(include=['object', 'category']).columns.to_list()\n",
    "\n",
    "datos[cat_cols] = datos[cat_cols].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Reiniciar Separar datos por tipos numericas <========================\n",
    "\n",
    "numeric_cols = datos.select_dtypes(include=['int16', 'int32', 'int64', 'float16', 'float32', 'float64']).columns.to_list()\n",
    "\n",
    "datos[numeric_cols] = datos[numeric_cols].astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Reiniciar datos numericas a categoricas <============================\n",
    "\n",
    "numeric_cols_cat_max = 1\n",
    "numeric_cols_cat = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if (datos[col].nunique() / datos[col].count()) * 100 <= numeric_cols_cat_max:\n",
    "        numeric_cols_cat.append(col) \n",
    "\n",
    "for col in numeric_cols_cat:\n",
    "    cat_cols.append(col)\n",
    "    numeric_cols.remove(col)\n",
    "\n",
    "datos[cat_cols] = datos[cat_cols].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Tratamiento de valores atípicos <====================================\n",
    "\n",
    "datos[numeric_cols] = datos[numeric_cols][~((datos[numeric_cols] < (intercuartilico[\"Q1\"] - 1.5 * intercuartilico[\"IQR\"])) |(datos[numeric_cols] > (intercuartilico[\"Q3\"] + 1.5 * intercuartilico[\"IQR\"]))).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Eliminar por Matriz de correlación <=================================\n",
    "\n",
    "drop_cols_corr = []\n",
    "\n",
    "corr_matrix = datos[numeric_cols].corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool_))\n",
    "\n",
    "drop_cols_corr = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "datos.drop(columns=drop_cols_corr, inplace=True)\n",
    "\n",
    "for col in drop_cols_corr:\n",
    "    numeric_cols.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Imputar nulos para tipos numericas <=================================\n",
    "\n",
    "datos[numeric_cols] = imp_num.transform(datos[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Reiniciar Separar datos por tipos categoricas <======================\n",
    "\n",
    "cat_cols = datos.select_dtypes(include=['object', 'category']).columns.to_list()\n",
    "cat_cols.remove('MachineIdentifier')\n",
    "\n",
    "datos[cat_cols] = datos[cat_cols].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Reiniciar Separar datos por tipos numericas <========================\n",
    "\n",
    "numeric_cols = datos.select_dtypes(include=['int16', 'int32', 'int64', 'float16', 'float32', 'float64']).columns.to_list()\n",
    "\n",
    "datos[numeric_cols] = datos[numeric_cols].astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Reiniciar datos numericas a categoricas <============================\n",
    "\n",
    "numeric_cols_cat_max = 1\n",
    "numeric_cols_cat = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if (datos[col].nunique() / datos[col].count()) * 100 <= numeric_cols_cat_max:\n",
    "        numeric_cols_cat.append(col) \n",
    "\n",
    "for col in numeric_cols_cat:\n",
    "    cat_cols.append(col)\n",
    "    numeric_cols.remove(col)\n",
    "\n",
    "datos[cat_cols] = datos[cat_cols].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Ordenar variables <==================================================\n",
    "\n",
    "ordered_columns = cat_cols+numeric_cols\n",
    "datos = datos[['MachineIdentifier']ordered_columns+['HasDetections']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================> Guardamos los datos preprocesados <==================================\n",
    "\n",
    "datos.to_csv('../data/test_pre.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
